{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline Preparation\n",
    "Follow the instructions below to help you create your ETL pipeline.\n",
    "### 1. Import libraries and load datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(messages_filepath, categories_filepath):\n",
    "    \"\"\"Reads the data from messages and categories files\n",
    "\n",
    "    Args:\n",
    "    messages_filepath: csv. The file that contains the disaster messages\n",
    "    categories_filepath: csv. The file that contains the categories each messages falls into \n",
    "    (36 categories in total - each messages is assigned 1 or 0 for each category)\n",
    "\n",
    "    Returns:\n",
    "    A dataframe which is a merge of the two input files.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = pd.read_csv(messages_filepath)\n",
    "    categories = pd.read_csv(categories_filepath)\n",
    "    \n",
    "    return pd.merge(messages,categories,on='id')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clean the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \n",
    "    \"\"\"Cleans the merged dataframe that the previous function created\n",
    "\n",
    "    Args:\n",
    "    df: The dataframe to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    The clean dataframe where a separate column for each category has been created and assigned 0-1 values \n",
    "    \"\"\"\n",
    "    \n",
    "    categories = df.categories.str.split(pat=';',expand=True)  # creates a column for each category (therefore a dataframe) \n",
    "    \n",
    "    # the values for these columns are of the form category-x (x=0-1) (e.g related-0, request-1)\n",
    "    # the first row of the new dataframe will be used to extract the column names for the categories\n",
    "    row = categories.iloc[0]  \n",
    "\n",
    "    \n",
    "    # apply lambda function that takes everything up to the second to last character of each string with slicing\n",
    "    category_colnames = row.apply(lambda x: x[:-2])   \n",
    "    \n",
    "    \n",
    "    categories.columns = category_colnames  # rename the columns of `categories`\n",
    "\n",
    "    \n",
    "    for column in categories.columns:\n",
    "        categories[column] = categories[column].apply(lambda x: x[len(x)-1:len(x)])  # extract the number at the end (0-1) \n",
    "    \n",
    "    \n",
    "    for column in categories.columns:\n",
    "        categories[column] = pd.to_numeric(categories[column])  # convert column from string to numeric\n",
    "    \n",
    "    \n",
    "    df.drop('categories',axis=1,inplace=True)  # drop the original categories column from `df`\n",
    "    \n",
    "    \n",
    "    df=pd.concat([df,categories],axis=1)  # concatenate the original dataframe with the new `categories` dataframe\n",
    "\n",
    "    \n",
    "    df.drop_duplicates(inplace=True)  # drop duplicates\n",
    "\n",
    "    # some categories were found to have a value of 2. They should change to 1 - lambda function applied\n",
    "    for column in categories.columns:\n",
    "        df[column]=df[column].apply(lambda x: 1 if x > 1 else x)    \n",
    "        \n",
    " \n",
    "    return df \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save the clean dataset into an sqlite database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, database_filename):\n",
    "    \"\"\"Saves the clean dataset as a table in a specific sqlite database (Disaster.db)\n",
    "\n",
    "    Args:\n",
    "    df: The cleaned dataframe \n",
    "    database_filename: The desired name of the table within the database\n",
    "   \n",
    "    Returns:\n",
    "    It doesn't need to return anything as it just stores the dataset in the database table.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    engine = create_engine('sqlite:///Disaster.db')  # create a connection to the database Disaster.db\n",
    "    df.to_sql(database_filename, engine, index=False,if_exists='replace')  # save the dataframe as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=load_data('messages.csv', 'categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean=clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(df_clean, 'Disaster_categ_messages')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
